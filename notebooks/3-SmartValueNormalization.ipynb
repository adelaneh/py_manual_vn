{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering-based Value Normalization (Smart Clustering)\n",
    "\n",
    "This notebook will cover using the ```py_valuenormalization``` package to cluster a set of values using a smart clustering algorithm and then clean up the resulting clusters.\n",
    "\n",
    "To use this package, first import it by running the following python command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py_valuenormalization as vn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load your data values into a list. We have prepared 4 datasets in the following files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = !ls -1 ../py_valuenormalization/data/\n",
    "tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file contains values to be normalized, one data value per line. Each of these datasets can be loaded using the following commands (here we load ```big_ten.txt```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = vn.read_from_file('../py_valuenormalization/data/big_ten.txt')\n",
    "\n",
    "from random import sample\n",
    "\n",
    "sample(vals, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we cluster the input values using a smart clustering algorithm, which finds the best parameter settings for standard hierarchical agglomerative clustering (HAC) using input training data.\n",
    "\n",
    "To obtain the training data, run the following command. It opens a window which directs you through varoius steps to create training data for smart clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(_, training_pairs) = vn.calibrate_normalization_cost_model(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where training_pairs is a dictionary where each key is a value pair (v1, v2) with v1 and v2 being distinct input values, and the corresponding value is True if v1 and v2 refer to the same entity and False otherwise.\n",
    "\n",
    "Now we cluster the input values using the smart clustering method and the training data in ```training_pairs``` using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smc = vn.SmartClustering(vals, training_pairs)\n",
    "\n",
    "(clusts, best_setting) = smc.cluster()\n",
    "\n",
    "(agrscore, simk, lnk, thr) = best_setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of a dictionary ```clusts``` and a tuple ```best_setting```. Each key of the dictionary ```clusts``` is the label of a cluster of data values, and the corresponding value is the set of data values in this cluster. ```best_setting = (agrscore, simk, lnk, thr)``` is a tuple of agreement score and HAC parameter settings using which ```clusts``` is obtained. ```agrscore``` is the agreement score between ```clusts``` and ```training_pairs```; i.e. the fraction of the value pairs in ```training_pairs``` which agree with ```clusts```. ```sim_measure```, ```linkage``` and ```thr``` are the standard HAC parameters settings using which ```clusts``` is obtained.\n",
    "\n",
    "Let's take a peak at the resulting clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(kk,vv) for (kk, vv) in clusts.items()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if there are any mixed clusters in ```clusts```, you can clean them up to arrive at the correct clustering of the input values. This phase consists of two main steps:\n",
    "\n",
    "1. Split step, where you split clusters containing values referring to more than one real-world entity into smaller clusters each of which contains values referring to a single entity\n",
    "2. Merge steps, in which you merge clusters referring to the same entity\n",
    "\n",
    "To clean up the clustering results run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_clusts = vn.normalize_clusters(clusts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ```clusts``` is a dictionary where each key is the label of a cluster of data values, and the corresponding value is the set of data values in this cluster. This will open a graphical user interface to clean up ```clusts``` and the results with be returned in ```clean_clusts``` which is a dictionary where each key is the label of a cluster of data values, and the corresponding value is the set of data values in this cluster.\n",
    "\n",
    "Let's take a peak at the final clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(kk,vv) for (kk, vv) in clean_clusts.items()][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
